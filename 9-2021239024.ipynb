{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3xOK_hjk1ifn",
      "metadata": {
        "id": "3xOK_hjk1ifn"
      },
      "source": [
        "# PyTorch Notebook for External Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Section 1: PyTorch Basics â€“ Tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0eec0cc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0eec0cc6",
        "outputId": "49a4c87d-6cff-44b6-e453-17375192b6f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pGWEtHKq1ifp",
      "metadata": {
        "id": "pGWEtHKq1ifp"
      },
      "source": [
        "**Q1.** What is a `torch.Tensor`? How is it different from a NumPy array?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a859b7",
      "metadata": {
        "id": "16a859b7"
      },
      "source": [
        "**Definition :** A tensor is a multi-dimensional array that is the core data structure in PyTorch.\n",
        "It stores data in numerical form (like scalars, vectors, matrices, or higher dimensions) and supports efficient computation on CPU and GPU.\n",
        "\n",
        "**How its different from numpy arrays :**\n",
        "\n",
        "- It has GPU support, means it can utilize GPU's memory to store tensors thus giving faster computation.\n",
        "- Allows easier gradient calculation (derivatives) which is not available in numpy\n",
        "- Tensors are designed to be used in deep learning (neural networks), unless numpy arrays which are primarily for numerical computations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "epnIxz1C1ifp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epnIxz1C1ifp",
        "outputId": "c75a2b86-3d6f-4a78-9d59-19b1b8d7f1d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1, 2, 3, 4, 5]), array([1, 2, 3, 4, 5]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Q1 Code Task: Create a 1D tensor with values [1, 2, 3, 4, 5]\n",
        "import torch\n",
        "import numpy\n",
        "\n",
        "tensor_1d = torch.tensor([1, 2, 3, 4, 5])\n",
        "numpy_1d = numpy.array([1, 2, 3, 4, 5])\n",
        "\n",
        "tensor_1d, numpy_1d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pf15lQVz1ifp",
      "metadata": {
        "id": "Pf15lQVz1ifp"
      },
      "source": [
        "**Q2.** How can you convert a PyTorch tensor to a NumPy array and vice versa?\n",
        "\n",
        "1.  To convert tensor to numpy array : Use the built in `.numpy()` method that comes with every tensor object.\n",
        "2.  To convert numpy array to tensor : Use the `from_numpy()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90l5CH4s1ifp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90l5CH4s1ifp",
        "outputId": "7357b74d-6556-43c4-80cb-be10b5d7ae32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "# Q2 Code Task: Convert the tensor [10, 20, 30] into a NumPy array and back to a tensor.\n",
        "\n",
        "num_from_ten = tensor_1d.numpy()\n",
        "print(type(num_from_ten))\n",
        "\n",
        "ten_from_num = torch.from_numpy(numpy_1d)\n",
        "print(type(ten_from_num))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VqYwrVD51ifq",
      "metadata": {
        "id": "VqYwrVD51ifq"
      },
      "source": [
        "**Q3.** Create a **2x3 tensor** filled with random numbers between 0 and 1. Print its shape and data type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bCyykASb3G5M",
      "metadata": {
        "id": "bCyykASb3G5M"
      },
      "source": [
        "There are many ways to create tensors with random values, which are listed as follows :\n",
        "\n",
        "- rand(x, y) - tensor of shape with random float values between 0 and 1\n",
        "- randn(x, y) - same with normal distribution\n",
        "- zeros(z, y) - fill with all 0\n",
        "- randint(low, high, size=(x, y)) - fills with random integer values within specified range\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "04DI9AYf1ifq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04DI9AYf1ifq",
        "outputId": "b1bb2b71-5c07-4a86-e1be-52a7e25b3ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.3014, 0.7377, 0.8437],\n",
            "        [0.7214, 0.6504, 0.6914]])\n",
            "torch.Size([2, 3])\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "# Q3 Code Task\n",
        "rand_tensor = torch.rand(2, 3)\n",
        "print(rand_tensor)\n",
        "print(rand_tensor.shape)\n",
        "print(rand_tensor.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yJMwbRA91ifq",
      "metadata": {
        "id": "yJMwbRA91ifq"
      },
      "source": [
        "**Q4.** Demonstrate element-wise addition and matrix multiplication with tensors. What is the difference between `*` and `@` operators in PyTorch?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qEUWDE0a4BaH",
      "metadata": {
        "id": "qEUWDE0a4BaH"
      },
      "source": [
        "Addition and Subtraction operations are simply done using the same operators used for scalar operations. Torch provides builtin functions too.\n",
        "\n",
        "When it comes to multiplication, since we are working with matrices there are two kind of multiplications.\n",
        "\n",
        "- Element-wise multiplication : `*` is used\n",
        "- Matrix multiplication : `@` is used\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C5n_cDDq1ifq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5n_cDDq1ifq",
        "outputId": "dc8159dd-7362-42fe-8493-7e78070b00fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 6,  8],\n",
            "        [10, 12]])\n",
            "tensor([[ 5, 12],\n",
            "        [21, 32]])\n",
            "tensor([[19, 22],\n",
            "        [43, 50]])\n"
          ]
        }
      ],
      "source": [
        "# Q4 Code Task: Perform element-wise addition and matrix multiplication on two 2x2 tensors.\n",
        "ten1 = torch.tensor([[1, 2], [3, 4]])\n",
        "ten2 = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "print(ten1 + ten2)  # torch.add()\n",
        "print(ten1 * ten2)  # torch.mul()\n",
        "print(ten1 @ ten2)  # torch.matmul()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "briwOctb1ifq",
      "metadata": {
        "id": "briwOctb1ifq"
      },
      "source": [
        "**Q5.** Explain broadcasting in PyTorch with an example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6Eq56q-H5Fm_",
      "metadata": {
        "id": "6Eq56q-H5Fm_"
      },
      "source": [
        "**Broadcasting** allows PyTorch to perform element-wise operations on tensors of different shapes, by automatically expanding the smaller tensor to match the shape of the larger tensor without copying data unnecessarily.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5c-c9VKM1ifq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c-c9VKM1ifq",
        "outputId": "0f17346d-3bac-4384-d57d-e358af5539d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2,  3,  4,  5],\n",
            "        [ 7,  8,  9, 10],\n",
            "        [12, 13, 14, 15]])\n"
          ]
        }
      ],
      "source": [
        "# Q5 Code Task: Add a tensor of shape (3,1) to a tensor of shape (3,4).\n",
        "ten1 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
        "ten2 = torch.tensor([[1], [2], [3]])\n",
        "\n",
        "res = ten1 + ten2\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "agN1PL941ifr",
      "metadata": {
        "id": "agN1PL941ifr"
      },
      "source": [
        "**Q6.** What is the difference between `view()` and `reshape()` in PyTorch?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nsJEmnWG5_Vj",
      "metadata": {
        "id": "nsJEmnWG5_Vj"
      },
      "source": [
        "**view**\n",
        "\n",
        "**Purpose:** Returns a new tensor with the same data but a different shape.\n",
        "**Important:**\n",
        "\n",
        "- The tensor must be contiguous in memory.\n",
        "- If itâ€™s not contiguous, view will throw an error.\n",
        "\n",
        "**reshape**\n",
        "\n",
        "**Purpose:** Returns a tensor with a different shape, just like view.\n",
        "**Key difference:**\n",
        "\n",
        "- reshape can handle non-contiguous tensors.\n",
        "- If the memory is not contiguous, it will make a copy under the hood to return the desired shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S7xb5k4u1ifr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7xb5k4u1ifr",
        "outputId": "4f47c5d3-3276-41f0-d953-33ba63eb3408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tensor (2x3):\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Shape: torch.Size([2, 3])\n",
            "\n",
            "Reshaped Tensor (3x2):\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "Shape: torch.Size([3, 2])\n"
          ]
        }
      ],
      "source": [
        "# Q6 Code Task: Create a tensor of shape (2,3) and reshape it to (3,2).\n",
        "\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "print(\"Original Tensor (2x3):\")\n",
        "print(tensor)\n",
        "print(\"Shape:\", tensor.shape)\n",
        "\n",
        "reshaped_tensor = tensor.view(3, 2)  # or tensor.reshape(3,2)\n",
        "print(\"\\nReshaped Tensor (3x2):\")\n",
        "print(reshaped_tensor)\n",
        "print(\"Shape:\", reshaped_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rIdo7S-y1ifr",
      "metadata": {
        "id": "rIdo7S-y1ifr"
      },
      "source": [
        "**Q7.** How do you check if a tensor is allocated on **CPU or GPU**?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ETf8PfuQ6nuL",
      "metadata": {
        "id": "ETf8PfuQ6nuL"
      },
      "source": [
        "Every tensor object has a `.device` property which tells you where it is stored - CPU or GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wbqjlsqT1ifr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbqjlsqT1ifr",
        "outputId": "73162dc3-88da-453c-cbad-2f3a8d5144d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Q7 Code Task: Create a tensor and move it to GPU (if available).\n",
        "\n",
        "# Tensor on CPU\n",
        "a = torch.tensor([1, 2, 3])\n",
        "print(a.device)  # Output: cpu\n",
        "\n",
        "# Tensor on GPU (if available)\n",
        "if torch.cuda.is_available():\n",
        "    b = a.to(\"cuda\")\n",
        "    print(b.device)  # Output: cuda:0 (first GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Rmyn-Sj1ifr",
      "metadata": {
        "id": "0Rmyn-Sj1ifr"
      },
      "source": [
        "**Q8.** Create an **identity matrix** of size 4x4 in PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jf1h-t7H7g2Q",
      "metadata": {
        "id": "Jf1h-t7H7g2Q"
      },
      "source": [
        "`torch.eye(n)` allows to create identity matrix of size n in torch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bW4zlqQH1ifr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW4zlqQH1ifr",
        "outputId": "12b129b8-29b4-418b-d36c-a08b708b03d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# Q8 Code Task\n",
        "I4 = torch.eye(4)\n",
        "print(I4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PkVRzvpO1ifr",
      "metadata": {
        "id": "PkVRzvpO1ifr"
      },
      "source": [
        "**Q9.** How do you find the maximum, minimum, and mean values of a tensor?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UzpZNNrT71UF",
      "metadata": {
        "id": "UzpZNNrT71UF"
      },
      "source": [
        "Torch provides built-in functions to calculate all these. Remember for mean, the function can only be called on tensor of type float.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dNJJxxBy1ifr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNJJxxBy1ifr",
        "outputId": "f94b46e1-616a-4f90-823b-d2d03d2503ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(9)\n",
            "tensor(2)\n",
            "tensor(5.4000)\n"
          ]
        }
      ],
      "source": [
        "# Q9 Code Task: Compute max, min, mean of tensor [4, 7, 9, 2, 5].\n",
        "ten = torch.tensor([4, 7, 9, 2, 5])\n",
        "\n",
        "print(ten.max())\n",
        "print(ten.min())\n",
        "print(ten.float().mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H44fE8YB1ifr",
      "metadata": {
        "id": "H44fE8YB1ifr"
      },
      "source": [
        "**Q10.** Explain slicing and indexing in tensors with an example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2mLWASrB8eDD",
      "metadata": {
        "id": "2mLWASrB8eDD"
      },
      "source": [
        "**Indexing :** You can access individual elements using square brackets [ ].\n",
        "\n",
        "**Slicing :** Slicing uses the start:stop:step syntax, just like Python lists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDrILsdk1ifs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDrILsdk1ifs",
        "outputId": "141c5c0e-fdda-48c2-cb89-e10557cd35aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tensor:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "\n",
            "First Row:\n",
            "tensor([1, 2, 3])\n",
            "\n",
            "Last Column:\n",
            "tensor([3, 6, 9])\n"
          ]
        }
      ],
      "source": [
        "# Q10 Code Task: Create a 3x3 tensor and extract the first row and last column.\n",
        "\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "print(\"Original Tensor:\")\n",
        "print(tensor)\n",
        "\n",
        "first_row = tensor[0, :]\n",
        "print(\"\\nFirst Row:\")\n",
        "print(first_row)\n",
        "\n",
        "last_column = tensor[:, -1]\n",
        "print(\"\\nLast Column:\")\n",
        "print(last_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v4UqmIvQ1ifs",
      "metadata": {
        "id": "v4UqmIvQ1ifs"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 2: Autograd & Gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IlyNT7Bh1ifs",
      "metadata": {
        "id": "IlyNT7Bh1ifs"
      },
      "source": [
        "**Q11.** What is autograd in PyTorch? Why is it useful?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l2DYJ63C8-B5",
      "metadata": {
        "id": "l2DYJ63C8-B5"
      },
      "source": [
        "**Autograd** in PyTorch is a module that automatically calculates gradients (derivatives) for tensors that have `requires_grad=True`.\n",
        "\n",
        "- In deep learning, we need to compute gradients of a loss with respect to model parameters (weights & biases).\n",
        "- Doing this manually is tedious and error-prone.\n",
        "- Autograd does it automatically, so you can focus on building models instead of calculating derivatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CtbD3JWq1ifs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtbD3JWq1ifs",
        "outputId": "a61a3bc8-8ec2-4ae5-a0bc-43278e978dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y :  tensor([9.], grad_fn=<PowBackward0>)\n",
            "Gradient of y :  tensor([6.])\n"
          ]
        }
      ],
      "source": [
        "# Q11 Code Task: Create a tensor `x` with requires_grad=True and compute gradient of y = x**2\n",
        "\n",
        "x = torch.tensor([3.0], requires_grad=True)\n",
        "y = x**2\n",
        "\n",
        "y.backward()  # Computes the gradients dy/dx\n",
        "\n",
        "print(\"y : \", y)\n",
        "print(\"Gradient of y : \", x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IIIpOcTp1ifs",
      "metadata": {
        "id": "IIIpOcTp1ifs"
      },
      "source": [
        "**Q12.** Explain the difference between `.backward()` and `.detach()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ip5Ggma59957",
      "metadata": {
        "id": "ip5Ggma59957"
      },
      "source": [
        "`.backward()`\n",
        "\n",
        "**Purpose:** Computes gradients of a tensor w.r.t. some scalar (usually a loss)\n",
        "\n",
        "**Used for:** Training neural networks, updating weights\n",
        "\n",
        "**Requirements:** The tensor must have requires_grad=True\n",
        "\n",
        "`.detach()`\n",
        "\n",
        "**Purpose:** Creates a new tensor that shares the same data but is not tracked by Autograd\n",
        "\n",
        "**Used for:**\n",
        "\n",
        "- Temporarily stopping gradient computation\n",
        "- Avoiding memory overhead during inference\n",
        "\n",
        "**Effect:** The new tensor wonâ€™t track operations, so .backward() wonâ€™t propagate through it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2YljpEvd1ifs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YljpEvd1ifs",
        "outputId": "acedf963-5ffd-4271-f002-bf6107bd8063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before detaching: requires_grad = True\n",
            "After detaching: requires_grad = False\n"
          ]
        }
      ],
      "source": [
        "# Q12 Code Task: Show how to stop gradient tracking for a tensor.# Create a tensor with gradient tracking enabled\n",
        "\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "print(\"Before detaching: requires_grad =\", x.requires_grad)\n",
        "\n",
        "# Stop gradient tracking\n",
        "x_detached = x.detach()\n",
        "print(\"After detaching: requires_grad =\", x_detached.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WjQppw3y1ifs",
      "metadata": {
        "id": "WjQppw3y1ifs"
      },
      "source": [
        "**Q13.** Compute gradients for y = 3x^3 + 2x^2 + 5 at x=2 using autograd.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ep-O5WX81ifs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep-O5WX81ifs",
        "outputId": "b366785f-fd1c-4741-bdc2-cc88ebc4f87b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient of y :  tensor([44.])\n"
          ]
        }
      ],
      "source": [
        "# Q13 Code Task\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "y = 3 * x**3 + 2 * x**2 + 5\n",
        "\n",
        "y.backward()\n",
        "\n",
        "print(\"Gradient of y : \", x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RcGEh4w61ifs",
      "metadata": {
        "id": "RcGEh4w61ifs"
      },
      "source": [
        "**Q14.** What happens if you call `.backward()` on a tensor without `requires_grad=True`?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CIsyQ35__P9m",
      "metadata": {
        "id": "CIsyQ35__P9m"
      },
      "source": [
        "If you try to call .backward() on a tensor that does not have `requires_grad=True`, PyTorch will raise an error (for non-scalar tensors) or simply do nothing for scalar tensors without gradients depending on the situation.\n",
        "\n",
        "**Reason:** PyTorchâ€™s Autograd only tracks operations on tensors with `requires_grad=True`.\n",
        "\n",
        "Without it, thereâ€™s no computational graph, so gradients cannot be computed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k6oRbqCk1ifs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6oRbqCk1ifs",
        "outputId": "78272057-8433-43ad-f84c-0234cfbc1cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ],
      "source": [
        "# Q14 Code Task: Demonstrate the error with an example.\n",
        "\n",
        "x = torch.tensor(2.0)  # requires_grad=False by default\n",
        "y = x**2\n",
        "\n",
        "# Try to compute gradient\n",
        "try:\n",
        "    y.backward()\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59rGNh1V1ifs",
      "metadata": {
        "id": "59rGNh1V1ifs"
      },
      "source": [
        "**Q15.** Perform gradient descent on f(w) = (w-3)^2 for 10 iterations with learning rate 0.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wd-E2aHc1ift",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd-E2aHc1ift",
        "outputId": "a04a7528-0885-4349-bf11-5634af28482b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1: w = 0.6000000238418579, f(w) = 9.0\n",
            "Iteration 2: w = 1.0800000429153442, f(w) = 5.760000228881836\n",
            "Iteration 3: w = 1.4639999866485596, f(w) = 3.6863999366760254\n",
            "Iteration 4: w = 1.7711999416351318, f(w) = 2.3592960834503174\n",
            "Iteration 5: w = 2.0169599056243896, f(w) = 1.5099495649337769\n",
            "Iteration 6: w = 2.2135679721832275, f(w) = 0.9663678407669067\n",
            "Iteration 7: w = 2.370854377746582, f(w) = 0.6184753179550171\n",
            "Iteration 8: w = 2.4966835975646973, f(w) = 0.39582422375679016\n",
            "Iteration 9: w = 2.597346782684326, f(w) = 0.2533273994922638\n",
            "Iteration 10: w = 2.677877426147461, f(w) = 0.16212961077690125\n"
          ]
        }
      ],
      "source": [
        "# Q15 Code Task\n",
        "\n",
        "w = torch.tensor([0.0], requires_grad=True)\n",
        "learning_rate = 0.1\n",
        "\n",
        "for i in range(10):\n",
        "    f = (w - 3) ** 2\n",
        "\n",
        "    f.backward()\n",
        "\n",
        "    with torch.no_grad():  # temporarily stop tracking\n",
        "        w -= learning_rate * w.grad\n",
        "\n",
        "    w.grad.zero_()\n",
        "\n",
        "    print(f\"Iteration {i+1}: w = {w.item()}, f(w) = {f.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q7Q6ApKt1ift",
      "metadata": {
        "id": "q7Q6ApKt1ift"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 3: Building Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zqxCRYle1ift",
      "metadata": {
        "id": "zqxCRYle1ift"
      },
      "source": [
        "**Q16.** What is `torch.nn.Module` and why is it useful?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Lz1B4qCANrA",
      "metadata": {
        "id": "0Lz1B4qCANrA"
      },
      "source": [
        "`torch.nn.Module` is a **base class** in PyTorch that all **neural network models** inherit from.\n",
        "\n",
        "It provides a convenient way to define layers, parameters, and the forward pass of a model, and automatically tracks **learnable parameters** for optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ScOM2INq1ift",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScOM2INq1ift",
        "outputId": "d0a992d8-63cc-4868-80b1-6051bda63a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Linear(in_features=1, out_features=1, bias=True)\n",
            "Weight: Parameter containing:\n",
            "tensor([[-0.3079]], requires_grad=True)\n",
            "Bias: Parameter containing:\n",
            "tensor([0.0311], requires_grad=True)\n",
            "Predicted y: tensor([[-0.5847]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Q16 Code Task: Define a simple linear model y = Wx + b using torch.nn.Linear\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input and output dimensions\n",
        "input_dim = 1  # x has 1 feature\n",
        "output_dim = 1  # y has 1 output\n",
        "\n",
        "# Define the linear model\n",
        "linear_model = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "# Print model parameters\n",
        "print(\"Model:\", linear_model)\n",
        "print(\"Weight:\", linear_model.weight)\n",
        "print(\"Bias:\", linear_model.bias)\n",
        "\n",
        "# Example: Forward pass\n",
        "x = torch.tensor([[2.0]])\n",
        "y_pred = linear_model(x)\n",
        "print(\"Predicted y:\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ROSV20nD1ift",
      "metadata": {
        "id": "ROSV20nD1ift"
      },
      "source": [
        "**Q17.** Create a feedforward neural network with 2 input features, 1 hidden layer (size=4, ReLU), and 1 output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87UHy7g61ift",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87UHy7g61ift",
        "outputId": "dde89722-3e07-47e8-d9f7-16a847a02bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN(\n",
            "  (hidden): Linear(in_features=2, out_features=4, bias=True)\n",
            "  (output): Linear(in_features=4, out_features=1, bias=True)\n",
            ")\n",
            "Predicted y: tensor([[0.4487]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Q17 Code Task\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        # Input layer to hidden layer\n",
        "        self.hidden = nn.Linear(2, 4)  # 2 input features, 4 hidden neurons\n",
        "        # Hidden layer to output\n",
        "        self.output = nn.Linear(4, 1)  # 1 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden(x))  # ReLU activation on hidden layer\n",
        "        x = self.output(x)  # Output layer (no activation)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = FeedForwardNN()\n",
        "print(model)\n",
        "\n",
        "# Example forward pass\n",
        "x = torch.tensor([[1.0, 2.0]])\n",
        "y_pred = model(x)\n",
        "print(\"Predicted y:\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sxyWl2Qh1if3",
      "metadata": {
        "id": "sxyWl2Qh1if3"
      },
      "source": [
        "**Q18.** Explain the role of activation functions. Implement ReLU and Sigmoid manually in PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HzwRG6-oB_Y8",
      "metadata": {
        "id": "HzwRG6-oB_Y8"
      },
      "source": [
        "**Role of Activation Functions**\n",
        "\n",
        "**1. Introduce Non-Linearity**\n",
        "\n",
        "- Without activation functions, a neural network with multiple layers would behave like a single linear layer.\n",
        "- Non-linearity allows the network to model complex patterns.\n",
        "\n",
        "**2. Control Output Range**\n",
        "\n",
        "- Some activations (like Sigmoid) squash outputs between 0 and 1.\n",
        "- Useful for probabilities, classification, etc.\n",
        "\n",
        "**3. Enable Learning**\n",
        "\n",
        "- Activations like ReLU help avoid vanishing gradients in deep networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p8G5hmJZ1if4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8G5hmJZ1if4",
        "outputId": "99ef4983-dea3-492b-d332-3c06cac63f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: tensor([-2.0000, -0.5000,  0.0000,  0.5000,  2.0000])\n",
            "ReLU output: tensor([0.0000, 0.0000, 0.0000, 0.5000, 2.0000])\n",
            "Sigmoid output: tensor([0.1192, 0.3775, 0.5000, 0.6225, 0.8808])\n"
          ]
        }
      ],
      "source": [
        "# Q18 Code Task: Define functions relu(x) and sigmoid(x) using tensors.\n",
        "\n",
        "x = torch.tensor([-2.0, -0.5, 0.0, 0.5, 2.0])\n",
        "\n",
        "\n",
        "# --- ReLU function ---\n",
        "def relu(x):\n",
        "    return torch.maximum(torch.tensor(0.0), x)\n",
        "\n",
        "\n",
        "# --- Sigmoid function ---\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "\n",
        "# Test the functions\n",
        "print(\"Input:\", x)\n",
        "print(\"ReLU output:\", relu(x))\n",
        "print(\"Sigmoid output:\", sigmoid(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PwnHXU9T1if4",
      "metadata": {
        "id": "PwnHXU9T1if4"
      },
      "source": [
        "**Q19.** What is the difference between `model.parameters()` and `model.state_dict()`?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fOEMu8SQCpiH",
      "metadata": {
        "id": "fOEMu8SQCpiH"
      },
      "source": [
        "**model.parameters()**\n",
        "\n",
        "- Returns an iterator over all learnable parameters (weights and biases) of the model.\n",
        "- These are nn.Parameter tensors that require gradients.\n",
        "- Typically used when defining an optimizer.\n",
        "\n",
        "**model.state_dict()**\n",
        "\n",
        "- Returns a Python dictionary that maps each layerâ€™s name to its parameters and buffers.\n",
        "- Includes: Weights and biases, Other stateful buffers (e.g., running mean/variance in BatchNorm)\n",
        "- Commonly used for saving and loading models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hUdMGO-81if4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUdMGO-81if4",
        "outputId": "31569f78-e882-473d-9772-e87f0aa062dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learnable parameters:\n",
            "Parameter containing:\n",
            "tensor([[-0.1711, -0.1451]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1956], requires_grad=True)\n",
            "\n",
            "Parameters with names:\n",
            "weight tensor([[-0.1711, -0.1451]])\n",
            "bias tensor([-0.1956])\n"
          ]
        }
      ],
      "source": [
        "# Q19 Code Task: Print the parameters of a small linear layer.\n",
        "\n",
        "linear_layer = nn.Linear(2, 1)\n",
        "\n",
        "print(\"Learnable parameters:\")\n",
        "for param in linear_layer.parameters():\n",
        "    print(param)\n",
        "\n",
        "print(\"\\nParameters with names:\")\n",
        "for name, param in linear_layer.state_dict().items():\n",
        "    print(name, param)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imIRddkt1if4",
      "metadata": {
        "id": "imIRddkt1if4"
      },
      "source": [
        "**Q20.** Implement forward pass of a 2-layer network without using nn.Module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZeiYQs5o1if4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeiYQs5o1if4",
        "outputId": "9e25cbc2-b0b5-4eb0-dc02-05d951dcf902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden layer output: tensor([[0.2690, 0.0000, 0.0000, 0.8984]])\n",
            "Network output: tensor([[-0.8663]])\n"
          ]
        }
      ],
      "source": [
        "# Q20 Code Task\n",
        "\n",
        "x = torch.tensor([[1.0, 2.0]])  # shape: [1, 2]\n",
        "\n",
        "# --- Initialize weights and biases manually ---\n",
        "# Layer 1 (input â†’ hidden)\n",
        "W1 = torch.randn(2, 4)  # weights for hidden layer\n",
        "b1 = torch.randn(4)  # bias for hidden layer\n",
        "\n",
        "# Layer 2 (hidden â†’ output)\n",
        "W2 = torch.randn(4, 1)  # weights for output layer\n",
        "b2 = torch.randn(1)  # bias for output layer\n",
        "\n",
        "\n",
        "# --- Define ReLU activation ---\n",
        "def relu(x):\n",
        "    return torch.maximum(torch.tensor(0.0), x)\n",
        "\n",
        "\n",
        "# --- Forward pass ---\n",
        "# Hidden layer computation\n",
        "hidden = relu(x @ W1 + b1)  # x @ W1 = matrix multiplication\n",
        "\n",
        "# Output layer computation\n",
        "output = hidden @ W2 + b2\n",
        "\n",
        "print(\"Hidden layer output:\", hidden)\n",
        "print(\"Network output:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qHp5-2l11if4",
      "metadata": {
        "id": "qHp5-2l11if4"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 4: Training a Simple Model (Logic Gates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dtV-LbQ1if4",
      "metadata": {
        "id": "6dtV-LbQ1if4"
      },
      "source": [
        "**Q21.** What is the purpose of a loss function? Give two common examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2_GpdxiDvBz",
      "metadata": {
        "id": "e2_GpdxiDvBz"
      },
      "source": [
        "A **loss function** (or cost function) measures how far the network's predictions are from the actual target values.\n",
        "\n",
        "- It quantifies the error between predicted outputs and true outputs.\n",
        "- The network tries to minimize this loss using optimization algorithms like gradient descent.\n",
        "\n",
        "Examples : Mean Squared Error, Cross-Entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6XJSw49h1if4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XJSw49h1if4",
        "outputId": "4720b6ef-dab4-4b48-dfb8-7612d28035b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE Loss: 0.25\n"
          ]
        }
      ],
      "source": [
        "# Q21 Code Task: Use torch.nn.MSELoss to compute loss between y_true=[1.0, 2.0] and y_pred=[1.5, 2.5].\n",
        "\n",
        "# True and predicted values\n",
        "y_true = torch.tensor([1.0, 2.0])\n",
        "y_pred = torch.tensor([1.5, 2.5])\n",
        "\n",
        "# Define the MSE loss\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# Compute the loss\n",
        "loss = mse_loss(y_pred, y_true)\n",
        "\n",
        "print(\"MSE Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JpBJKkrb1if4",
      "metadata": {
        "id": "JpBJKkrb1if4"
      },
      "source": [
        "**Q22.** What is the role of an optimizer in training neural networks?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xUhHactxEK_d",
      "metadata": {
        "id": "xUhHactxEK_d"
      },
      "source": [
        "An **optimizer** is an algorithm that updates the modelâ€™s parameters (weights and biases) based on the computed gradients to minimize the loss function.\n",
        "\n",
        "- During training, the network computes the gradient of the loss w.r.t. each parameter using Autograd.\n",
        "- The optimizer uses these gradients to adjust the parameters in the direction that reduces the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yTuSM0Tm1if4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTuSM0Tm1if4",
        "outputId": "68a0ff0c-66cc-4452-c290-7f0369be2f66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Q22 Code Task: Define SGD optimizer for a linear model with learning rate=0.01.\n",
        "\n",
        "model = nn.Linear(2, 1)  # 2 input features â†’ 1 output\n",
        "\n",
        "# Define SGD optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Print optimizer to check\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K985biS81if4",
      "metadata": {
        "id": "K985biS81if4"
      },
      "source": [
        "**Q23.** Train a simple linear regression model to fit y = 2x + 1 for x in [1,2,3,4,5].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZRkNsrBz1if4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRkNsrBz1if4",
        "outputId": "856f47ee-29f9-4526-d25b-3173c0ee921c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100, Loss: 0.0645\n",
            "Epoch 200, Loss: 0.0327\n",
            "Epoch 300, Loss: 0.0166\n",
            "Epoch 400, Loss: 0.0085\n",
            "Epoch 500, Loss: 0.0043\n",
            "Epoch 600, Loss: 0.0022\n",
            "Epoch 700, Loss: 0.0011\n",
            "Epoch 800, Loss: 0.0006\n",
            "Epoch 900, Loss: 0.0003\n",
            "Epoch 1000, Loss: 0.0001\n",
            "Prediction for x=6: 13.018630027770996\n"
          ]
        }
      ],
      "source": [
        "# Q23 Code Task\n",
        "import torch.optim as optim\n",
        "\n",
        "x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])\n",
        "y_train = torch.tensor([[3.0], [5.0], [7.0], [9.0], [11.0]])\n",
        "\n",
        "model = nn.Linear(1, 1)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(x_train)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Example prediction\n",
        "x_test = torch.tensor([[6.0]])\n",
        "y_test = model(x_test)\n",
        "print(\"Prediction for x=6:\", y_test.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HEBgAVr21if5",
      "metadata": {
        "id": "HEBgAVr21if5"
      },
      "source": [
        "**Q24.** Implement and train a neural network for the AND gate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wH--0biG1if5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH--0biG1if5",
        "outputId": "1ee8852e-83f2-491f-e0ad-17df7cfe54ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 500, Loss: 0.5123\n",
            "Epoch 1000, Loss: 0.3470\n",
            "Epoch 1500, Loss: 0.1759\n",
            "Epoch 2000, Loss: 0.0933\n",
            "Epoch 2500, Loss: 0.0575\n",
            "Epoch 3000, Loss: 0.0398\n",
            "Epoch 3500, Loss: 0.0298\n",
            "Epoch 4000, Loss: 0.0235\n",
            "Epoch 4500, Loss: 0.0193\n",
            "Epoch 5000, Loss: 0.0162\n",
            "Predicted probabilities:\n",
            " tensor([[5.7623e-04],\n",
            "        [1.3765e-02],\n",
            "        [1.1081e-02],\n",
            "        [9.6140e-01]])\n",
            "Predicted classes:\n",
            " tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ],
      "source": [
        "# Q24 Code Task\n",
        "\n",
        "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "Y = torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)\n",
        "\n",
        "\n",
        "class ANDNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANDNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)  # 2 input features â†’ 2 hidden neurons\n",
        "        self.fc2 = nn.Linear(2, 1)  # 2 hidden neurons â†’ 1 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid activation on hidden layer\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation on output layer\n",
        "        return x\n",
        "\n",
        "\n",
        "model = ANDNet()\n",
        "\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "num_epochs = 5000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(y_pred, Y)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    predictions = model(X)\n",
        "    predicted_classes = (predictions > 0.5).float()\n",
        "    print(\"Predicted probabilities:\\n\", predictions)\n",
        "    print(\"Predicted classes:\\n\", predicted_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dozaTv7F1if5",
      "metadata": {
        "id": "dozaTv7F1if5"
      },
      "source": [
        "**Q25.** Implement and train a neural network for the XOR gate (with hidden layer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g8r554cp1if5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8r554cp1if5",
        "outputId": "2dcdc1af-dd2b-40bc-9577-dcd755040317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1000, Loss: 0.6932\n",
            "Epoch 2000, Loss: 0.6931\n",
            "Epoch 3000, Loss: 0.6931\n",
            "Epoch 4000, Loss: 0.6930\n",
            "Epoch 5000, Loss: 0.6929\n",
            "Epoch 6000, Loss: 0.6927\n",
            "Epoch 7000, Loss: 0.6920\n",
            "Epoch 8000, Loss: 0.6886\n",
            "Epoch 9000, Loss: 0.6612\n",
            "Epoch 10000, Loss: 0.5298\n",
            "Predicted probabilities:\n",
            " tensor([[0.4046],\n",
            "        [0.4255],\n",
            "        [0.7603],\n",
            "        [0.3758]])\n",
            "Predicted classes:\n",
            " tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "# Q25 Code Task\n",
        "\n",
        "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "\n",
        "\n",
        "class XORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XORNet, self).__init__()\n",
        "        self.hidden = nn.Linear(2, 2)  # 2 inputs â†’ 2 hidden neurons\n",
        "        self.output = nn.Linear(2, 1)  # 2 hidden neurons â†’ 1 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.hidden(x))  # hidden layer activation\n",
        "        x = torch.sigmoid(self.output(x))  # output layer activation\n",
        "        return x\n",
        "\n",
        "\n",
        "model = XORNet()\n",
        "\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy for binary classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "num_epochs = 10000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(y_pred, Y)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(X)\n",
        "    predicted_classes = (predictions > 0.5).float()\n",
        "    print(\"Predicted probabilities:\\n\", predictions)\n",
        "    print(\"Predicted classes:\\n\", predicted_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s1xPzOBYFbZD",
      "metadata": {
        "id": "s1xPzOBYFbZD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
